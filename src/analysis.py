import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict

from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

import scipy.cluster.hierarchy as sch

from sklearn.metrics import silhouette_score

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split

from pandas.api.types import is_numeric_dtype


# ============================================
# üåç Globals
# ============================================


SMALL_PATH = 'data/small.xlsx'
MEDIUM_PATH = 'data/medium.xlsx'
LARGE_PATH = 'data/large.xlsx'

DELETED_SMALL_PATH = 'out/data_with_gaps/small'
DELETED_MEDIUM_PATH = 'out/data_with_gaps/medium'
DELETED_LARGE_PATH = 'out/data_with_gaps/large'

PERCENTS_OF_GAPS = [0.03, 0.05, 0.1, 0.2, 0.3]


# ============================================
# üìÑ Dataset class
# ============================================


class Dataset():
    def __init__(self, path: str):
        self.df = self.load_data(path)       
        self.preprocess()
        
        
    #-------------Data Loading & Preprocess------------    
        
    def load_data(self, path: str) -> pd.DataFrame:
        try:
            df = pd.read_excel(path)
        except FileNotFoundError:
            print(f"File {path} not exists")
            df = None
        return df
    
    
    def preprocess(self):
        self.df['date-time'] = pd.to_datetime(self.df['date-time'])
        self.df['cards_number'] = self.df['cards_number'].astype('string')
        

    #--------------Analysis & Diagrams---------------
    
    def count(self):
        df_count = pd.DataFrame()
        
        for col in self.df.columns:
            if pd.api.types.is_numeric_dtype(self.df[col]):
                df_count[col] = [self.df[col].mean(), self.df[col].median(), list(self.df[col].mode())[:5]]
            elif pd.api.types.is_string_dtype(self.df[col]):
                df_count[col] = [None, None, list(self.df[col].mode())[0]]
                
        return df_count


    def count_all(self):
        df = self.df
        result = {}

        for col in df.columns:
            s = df[col]

            # –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º NaN / NaT, —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞–ª–∏ –ø–æ–¥—Å—á—ë—Ç—É
            s_no_na = s.dropna()

            # –µ—Å–ª–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ –≤–æ–æ–±—â–µ –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ dropna
            if s_no_na.empty:
                result[col] = [np.nan, np.nan, np.nan]
                continue

            # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
            if pd.api.types.is_numeric_dtype(s_no_na):
                mean_val = s_no_na.mean()
                median_val = s_no_na.median()
                mode_series = s_no_na.mode()
                mode_val = mode_series.iloc[0] if not mode_series.empty else np.nan

            # –î–∞—Ç—ã/–≤—Ä–µ–º—è ‚Äî –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ int64 (–Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã), —Å—á–∏—Ç–∞–µ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º –æ–±—Ä–∞—Ç–Ω–æ
            elif pd.api.types.is_datetime64_any_dtype(s_no_na):
                # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —á–∏—Å–ª–∞
                numeric = s_no_na.view('int64')

                mean_num = numeric.mean()
                median_num = numeric.median()
                mode_series = numeric.mode()
                mode_num = mode_series.iloc[0] if not mode_series.empty else np.nan

                # –ø–µ—Ä–µ–≤–æ–¥–∏–º –æ–±—Ä–∞—Ç–Ω–æ –≤ datetime
                mean_val = pd.to_datetime(mean_num)
                median_val = pd.to_datetime(median_num)
                mode_val = pd.to_datetime(mode_num) if not pd.isna(mode_num) else pd.NaT

            else:
                # –°—Ç—Ä–æ–∫–∏/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: —Ñ–∞–∫—Ç–æ—Ä–∏–∑—É–µ–º (—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —á–∏—Å–ª–∞)
                # factorize –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç codes –∏ –º–∞—Å—Å–∏–≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                codes, uniques = pd.factorize(s_no_na, sort=True)
                codes = pd.Series(codes, index=s_no_na.index)

                # —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ/–º–µ–¥–∏–∞–Ω—É/–º–æ–¥—É –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–¥–∞–º
                mean_code = int(round(codes.mean()))
                median_code = int(np.median(codes))
                mode_code = int(codes.mode().iloc[0])

                # –æ–≥—Ä–∞–Ω–∏—á–∏–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                max_idx = len(uniques) - 1
                mean_code = min(max(mean_code, 0), max_idx)
                median_code = min(max(median_code, 0), max_idx)
                mode_code = min(max(mode_code, 0), max_idx)

                # –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏
                mean_val = uniques[mean_code]
                median_val = uniques[median_code]
                mode_val = uniques[mode_code]

            result[col] = [mean_val, median_val, mode_val]

        df_count = pd.DataFrame(result, index=['mean', 'median', 'mode'])
        return df_count

    
    
    def draw_hist_store(self):
        counts = self.df['store_name'].value_counts()

        plt.figure(figsize=(10, 8))
        plt.barh(counts.index, counts.values)
        plt.xlabel("Number of rows")
        plt.ylabel("Store Name")
        plt.tight_layout()
        plt.show()
        
        
    def draw_hist_date(self):
        self.df['date-time'] = pd.to_datetime(self.df['date-time'])

        df_month = pd.DataFrame()
        df_month['month'] = self.df['date-time'].dt.month

        def get_season(m):
            if m in [12, 1, 2]:
                return "Winter"
            elif m in [3, 4, 5]:
                return "Spring"
            elif m in [6, 7, 8]:
                return "Summer"
            else:
                return "Autumn"

        df_month['season'] = df_month['month'].apply(get_season)

        plt.hist(df_month['season'])
        plt.xlabel("Season")
        plt.ylabel("Number of rows")
        plt.show()
        
        
    def draw_hist_coords(self):
        counts = self.df['coordinates'].value_counts()

        plt.figure(figsize=(12,5))
        plt.barh(counts.index, counts.values)
        plt.xlabel("Number of rows")
        plt.ylabel("Coordinates")
        plt.show()
        
        
    def draw_hist_cats(self):
        counts = self.df['categories'].value_counts()

        plt.figure(figsize=(10, 8))
        plt.barh(counts.index, counts.values)
        plt.xlabel("Number of rows")
        plt.ylabel("Category Name")
        plt.tight_layout()
        plt.show()
        
        
    def draw_hist_brand(self):
        counts = self.df['brands'].value_counts()
        
        plt.hist(counts, bins=30)
        plt.xlabel("Times the value occur")
        plt.ylabel("Number of values with this frequency")
        plt.title("Frequency distribution of categories")
        plt.show()
        
        
    def draw_hist_top_brands(self):
        counts = self.df['brands'].value_counts().head(10)
        
        plt.figure(figsize=(10,5))
        plt.bar(counts.index, counts.values)
        plt.xticks(rotation=45)
        plt.xlabel("Brands")
        plt.ylabel("Number of rows")
        plt.title("Top 10 Brands")
        plt.show()
        
        
    def draw_hist_bottom_brands(self):
        counts = self.df['brands'].value_counts().tail(10)
        
        plt.figure(figsize=(10,5))
        plt.bar(counts.index, counts.values)
        plt.xticks(rotation=45)
        plt.xlabel("Brands")
        plt.ylabel("Number of rows")
        plt.title("Top 10 Brands")
        plt.show()
        
        
    def draw_hist_price(self):
        plt.hist(self.df.loc[self.df['price'] <= 100000, 'price'], bins=50)
        plt.xlabel("Price")
        plt.ylabel("Number of rows")
        plt.show()
        
        
    def count_unique(self, feature: str):
        counts = self.df[feature].value_counts()
        return counts.describe()
    
    
    def draw_hist_num_products(self):
        counts = self.df['number_of_products'].value_counts().sort_index()
        counts.plot(kind='bar')
        plt.xlabel("Number_of_products")
        plt.ylabel("Number of rows")
        plt.xticks(rotation=0)
        plt.show()
        
        
    def analyse_receipt_id(self):
        print(f"Unique number: {self.df['receipt_id'].nunique()}, Number of rows: {len(self.df)}")
        print("Highest number of unique receipt id for stores:")
        print(self.df.groupby("store_name")["receipt_id"].nunique().head())
    
    
    def draw_hist_total_cost(self):
        plt.hist(self.df.loc[self.df['total_cost'] <= 100000, 'total_cost'], bins=50)
        plt.xlabel("Total cost")
        plt.ylabel("Number of rows")
        plt.show()
        
        
    #---------------------Make Gaps-----------------------
        
    def remove_blocks(self, percent: float=0.3, inplace: bool=True) -> pd.DataFrame:
        if inplace:
            df = self.df
        else:
            df = self.df.copy()
        
        rows, cols = df.shape
        total_cells = rows * cols
        target_remove = int(total_cells * percent)
        
        block_sizes = [(2, 2), (3, 3), (4, 4), (2, 3), (3, 2), (2, 4), (4, 2), (3, 4), (4, 3)]
        
        removed = 0
        
        while removed < target_remove:
            block_h, block_w = random.choice(block_sizes)
            
            r = random.randint(0, rows - block_h)
            c = random.randint(0, cols - block_w)
            
            for i in range(r, r + block_h):
                for j in range(c, c + block_w):
                    df.iat[i, j] = np.nan
                    
            removed += block_h * block_w
                    
        return df
    
    
    #----------------Restore Functions--------------------
    
    def inpute_groups(self, df: pd.DataFrame | None=None):
        if df is None:
            df = self.df
            
        countable_features = ['price', 'number_of_products', 'total_cost']
        string_features = ['store_name', 'date-time', 'coordinates', 'categories', 'brands', 'cards_number', 'receipt_id']
        
        groups = {
            "store_name": [['coordinates', 'cards_number'], ['cards_number', 'receipt_id'], ['cards_number', 'total_cost'], ['receipt_id', 'total_cost']],
            "date-time": [['store_name', 'receipt_id'], ['store_name', 'cards_number'], ['receipt_id', 'total_cost']],
            "coordinates": [['store_name', 'receipt_id'], ['date-time', 'receipt_id'], ['store_name', 'cards_number'], ['date-time', 'cards_number'], ['date-time', 'total_cost']],
            "categories": [['store_name', 'brands'], ['coordinates', 'brands'], ['store_name', 'price'], ['coordinates', 'price']],
            "brands": [['store_name', 'categories'], ['coordinates', 'categories'], ['store_name', 'price'], ['coordinates', 'price']],
            "price": [['categories', 'brands'], ['store_name', 'brands'], ['store_name', 'categories'], ['coordinates', 'brands'], ['coordinates', 'categories']],
            "cards_number": [['store_name', 'receipt_id'], ['date-time', 'receipt_id'], ['date-time', 'total_cost']],
            "number_of_products": [['store_name', 'receipt_id'], ['date-time', 'receipt_id'], ['store_name', 'total_cost'], ['date-time', 'total_cost'], ['cards_number', 'receipt_id']],
            "receipt_id": [['date-time', 'cards_number'], ['date-time', 'total_cost'], ['store_name', 'date-time'], ['store_name', 'total_cost'], ['cards_number', 'total_cost']],
            "total_cost": [['store_name', 'receipt_id'], ['date-time', 'receipt_id'], ['date-time', 'cards_number']]
        }
                       
                        
        def make_group(df: pd.DataFrame, features: list[str]):
            return df.dropna(subset=features).groupby(features)
        
        for i in range(2):
            for idx, col in df.isna().stack()[lambda x: x].index:
                row = df.loc[idx]
                
                for group in groups[col]:
                
                    if pd.notna(row[group[0]]) and pd.notna(row[group[1]]):
                        if col in string_features:
                            grp = make_group(df, group)
                            
                            s = grp[col].get_group((row[group[0]], row[group[1]])).dropna()
                            
                            if col == "cards_number":
                                s = s.astype(str)
                            
                            if s.empty:
                                val = pd.NA
                            else:
                                val = s.mode()
                                val=val.iloc[0]
                                
                            if col == "cards_number":
                                val = pd.NA if pd.isna(val) else str(val).replace('.0', '')
                            
                            df.at[idx, col] = val
                        else:
                            grp = make_group(df, group)
                            
                            s = grp[col].get_group((row[group[0]], row[group[1]])).dropna()
                            
                            if s.empty:
                                val = pd.NA
                            else:
                                val = s.median()
                            
                            df.at[idx, col] = val
                            
                        break          
        
        for idx, col in df.isna().stack()[lambda x: x].index:
                row = df.loc[idx]
                
                if col in string_features:
                    df.at[idx, col] = df[col].mode()[0]
                else:
                    df.at[idx, col] = df[col].median()
                    
        df['cards_number'] = df['cards_number'].astype('string').str.replace(r"\.0$", "", regex=True)
        
        return df
    
    
    def _build_zet_vectors(self, df: pd.DataFrame):
        from sklearn.feature_extraction.text import TfidfVectorizer
        from scipy.sparse import hstack, csr_matrix
        import numpy as np

        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        
        cat_cols = [c for c in df.columns if c not in num_cols]

        if "cards_number" in df.columns:
            df["cards_number"] = df["cards_number"].astype("string")

        def row_to_tokens(row):
            toks = []
            for c in cat_cols:
                val = row[c]
                if pd.isna(val):
                    toks.append(f"{c}=__MISSING__")
                else:
                    toks.append(f"{c}={str(val)}")
            return " ".join(toks)

        texts = df.apply(row_to_tokens, axis=1)

        self._zet_vectorizer = TfidfVectorizer(token_pattern=r"[^ ]+")
        X_tfidf = self._zet_vectorizer.fit_transform(texts)

        if len(num_cols) > 0:
            Z_list = []
            means = {}
            stds = {}

            for c in num_cols:
                col = df[c].astype(float)
                m = col.mean(skipna=True)
                s = col.std(skipna=True)
                
                if not np.isfinite(s) or s == 0:
                    s = 1.0
                z = (col - m) / s
                z = z.fillna(0.0)
                Z_list.append(z.to_numpy().reshape(-1, 1))
                means[c] = float(m)
                stds[c] = float(s)

            X_num = csr_matrix(np.hstack(Z_list))
            X = hstack([X_tfidf, X_num], format="csr")
        else:
            means, stds = {}, {}
            X = X_tfidf

        meta = {
            "num_cols": num_cols,
            "cat_cols": cat_cols,
            "means": means,
            "stds": stds,
            "row_index": df.index.to_numpy()
        }
        return X, meta


    def impute_zet(self, df: pd.DataFrame | None = None, k: int = 15, target_cols: list[str] | None = None):
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        import pandas as pd
        from collections import defaultdict

        if df is None:
            df = self.df.copy()
        else:
            df = df.copy()

        if "cards_number" in df.columns:
            df["cards_number"] = df["cards_number"].astype("string")

        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        cat_cols = [c for c in df.columns if c not in num_cols]

        X, meta = self._build_zet_vectors(df)

        if target_cols is None:
            cols_with_nan = [c for c in df.columns if df[c].isna().any()]
        else:
            cols_with_nan = [c for c in target_cols if c in df.columns and df[c].isna().any()]
        if not cols_with_nan:
            return df
        def weighted_mode(values, weights):
            score = defaultdict(float)
            for v, w in zip(values, weights):
                if pd.isna(v):
                    continue
                score[v] += float(w)
            if not score:
                return pd.NA
            return max(score.items(), key=lambda x: x[1])[0]

        for col in cols_with_nan:
            candidate_mask = df[col].notna().to_numpy()
            candidate_indices = np.where(candidate_mask)[0]
            if candidate_indices.size == 0:
                continue

            X_cand = X[candidate_indices]

            missing_idx = np.where(df[col].isna().to_numpy())[0]
            if missing_idx.size == 0:
                continue

            for i in missing_idx:
                x_i = X[i]
                sims = cosine_similarity(x_i, X_cand).ravel()
                if sims.size == 0:
                    continue
                top_k_idx = np.argpartition(-sims, min(k, sims.size - 1))[:k]
                neigh_sims = sims[top_k_idx]
                neigh_rows = candidate_indices[top_k_idx]

                total_w = float(neigh_sims.sum())
                if total_w <= 1e-12:
                    if col in num_cols:
                        fill_val = df[col].median()
                    else:
                        try:
                            fill_val = df[col].mode(dropna=True).iloc[0]
                        except Exception:
                            fill_val = pd.NA
                    df.iat[i, df.columns.get_loc(col)] = fill_val
                    continue

                if col in num_cols:
                    vals = df.iloc[neigh_rows][col].to_numpy(dtype=float)
                    fill_val = float((vals * neigh_sims).sum() / total_w)
                else:
                    vals = df.iloc[neigh_rows][col].astype("string").to_numpy()
                    fill_val = weighted_mode(vals, neigh_sims)

                    if col == "cards_number":
                        if pd.isna(fill_val):
                            pass
                        else:
                            fill_val = str(fill_val).replace(".0", "")

                df.iat[i, df.columns.get_loc(col)] = fill_val

        if "cards_number" in df.columns:
            df["cards_number"] = df["cards_number"].astype("string").str.replace(r"\.0$", "", regex=True)

        return df
    
    
    #------------Clusterization----------------
    # def hierarchical_clustering(
    #     self,
    #     features: list[str],
    #     n_clusters: int | None = 3,
    #     *,
    #     linkage: str = "ward",                # "ward" | "complete" | "average" | "single"
    #     metric: str = "euclidean",            # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è "ward" (–≤—Å–µ–≥–¥–∞ euclidean)
    #     distance_threshold: float | None = None,
    #     standardize: bool = True,
    #     label_column: str | None = None,
    #     plot: bool = True,
    #     savefig_path: str | None = None,
    # ):
    #     """
    #     –ê–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º.

    #     –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    #         features           : —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ DataFrame –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—á–∏—Å–ª–æ–≤—ã–µ).
    #         n_clusters         : —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤; –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω distance_threshold.
    #         linkage            : –ø—Ä–∞–≤–∏–ª–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
    #         metric             : –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–¥–ª—è "ward" –≤—Å–µ–≥–¥–∞ "euclidean").
    #         distance_threshold : –ø–æ—Ä–æ–≥ –≤—ã—Å–æ—Ç—ã –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
    #         standardize        : —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ (StandardScaler).
    #         label_column       : –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ self.df.
    #         plot               : —Ä–∏—Å–æ–≤–∞—Ç—å –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—É.
    #         savefig_path       : –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω).

    #     –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    #         (labels: pd.Series, model: AgglomerativeClustering)
    #     """
    #     # --- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
    #     X = self.df[features].dropna()
    #     num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    #     if not num_cols:
    #         raise ValueError("–ù—É–∂–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
    #     if len(num_cols) < len(X.columns):
    #         X = X[num_cols]

    #     X_values = X.values
    #     if standardize:
    #         X_values = StandardScaler().fit_transform(X_values)

    #     # --- –º–æ–¥–µ–ª—å ---
    #     effective_metric = "euclidean" if linkage == "ward" else metric
    #     model = AgglomerativeClustering(
    #         n_clusters=None if distance_threshold is not None else n_clusters,
    #         linkage=linkage,
    #         metric=effective_metric,
    #         distance_threshold=distance_threshold,
    #         compute_distances=distance_threshold is not None,
    #     ).fit(X_values)

    #     labels = pd.Series(model.labels_, index=X.index, name="cluster")

    #     # --- –∑–∞–ø–∏—Å—å –º–µ—Ç–æ–∫ –≤ df ---
    #     if label_column is None:
    #         suffix = (
    #             f"thr_{distance_threshold}"
    #             if distance_threshold is not None
    #             else f"{n_clusters}"
    #         )
    #         label_column = f"hclust_{linkage}_{suffix}"
    #     self.df[label_column] = pd.Series(index=self.df.index, dtype="Int64")
    #     self.df.loc[labels.index, label_column] = labels.values

    #     # --- –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞) ---
    #     if plot:
    #         Z = sch.linkage(X_values, method=linkage, metric=effective_metric)
    #         plt.figure(figsize=(10, 5))
    #         sch.dendrogram(Z, no_labels=True)
    #         plt.title("Hierarchical clustering dendrogram")
    #         plt.xlabel("Objects")
    #         plt.ylabel("Distance")
    #         if savefig_path:
    #             plt.tight_layout()
    #             plt.savefig(savefig_path, dpi=150)
    #         plt.show()

    #     return labels, model


    def hierarchical_clustering(
        self,
        features: list[str],
        n_clusters: int | None = 3,
        *,
        linkage: str = "ward",                # "ward" | "complete" | "average" | "single"
        metric: str = "euclidean",            # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è "ward" (–≤—Å–µ–≥–¥–∞ euclidean)
        distance_threshold: float | None = None,
        standardize: bool = True,
        label_column: str | None = None,      # –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –º–µ—Ç–æ–∫ –∏–∑ AgglomerativeClustering
        # --- –ù–û–í–û–ï ---
        cut_distance: float | None = None,    # –≤—ã—Å–æ—Ç–∞ –æ—Ç—Å–µ—á–µ–Ω–∏—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã, –Ω–∞–ø—Ä. 120
        cut_label_column: str | None = None,  # –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –º–µ—Ç–æ–∫ –ø—Ä–∏ cut_distance
        # -------------
        plot: bool = True,
        savefig_path: str | None = None,
        ):
        """
        –ê–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            features           : —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ DataFrame –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—á–∏—Å–ª–æ–≤—ã–µ).
            n_clusters         : —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤; –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω distance_threshold.
            linkage            : –ø—Ä–∞–≤–∏–ª–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
            metric             : –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–¥–ª—è "ward" –≤—Å–µ–≥–¥–∞ "euclidean").
            distance_threshold : –ø–æ—Ä–æ–≥ –≤—ã—Å–æ—Ç—ã –≤ sklearn-–∫–ª–∞—Å—Ç–µ—Ä–µ –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
            standardize        : —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ (StandardScaler).
            label_column       : –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (sklearn) –≤ self.df.
            cut_distance       : —É—Ä–æ–≤–µ–Ω—å –æ—Ç—Å–µ—á–µ–Ω–∏—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã (–ø–æ –º–∞—Ç—Ä–∏—Ü–µ Z, SciPy).
            cut_label_column   : –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ cut_distance.
            plot               : —Ä–∏—Å–æ–≤–∞—Ç—å –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—É.
            savefig_path       : –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω).

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            (labels: pd.Series, model: AgglomerativeClustering, cut_labels: pd.Series | None)
        """
        # --- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
        X = self.df[features].dropna()
        num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]

        if not num_cols:
            raise ValueError("–ù—É–∂–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
        if len(num_cols) < len(X.columns):
            X = X[num_cols]

        X_values = X.values
        if standardize:
            X_values = StandardScaler().fit_transform(X_values)

        # --- –º–æ–¥–µ–ª—å (sklearn) ---
        effective_metric = "euclidean" if linkage == "ward" else metric
        model = AgglomerativeClustering(
            n_clusters=None if distance_threshold is not None else n_clusters,
            linkage=linkage,
            metric=effective_metric,
            distance_threshold=distance_threshold,
            compute_distances=distance_threshold is not None,
        ).fit(X_values)

        labels = pd.Series(model.labels_, index=X.index, name="cluster")

        # --- –∑–∞–ø–∏—Å—å –º–µ—Ç–æ–∫ sklearn-–∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ df ---
        if label_column is None:
            suffix = (
                f"thr_{distance_threshold}"
                if distance_threshold is not None
                else f"{n_clusters}"
            )
            label_column = f"hclust_{linkage}_{suffix}"

        self.df[label_column] = pd.Series(index=self.df.index, dtype="Int64")
        self.df.loc[labels.index, label_column] = labels.values

        # --- SciPy linkage (–¥–ª—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—ã + cut_distance) ---
        cut_labels = None      # —Ç–æ, —á–µ–º –≤–µ—Ä–Ω—ë–º –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ cut_distance

        if plot or (cut_distance is not None):
            Z = sch.linkage(X_values, method=linkage, metric=effective_metric)

            # –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞
            if plot:
                plt.figure(figsize=(10, 5))
                sch.dendrogram(Z, no_labels=True)
                if distance_threshold is not None:
                    # –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ distance_threshold (–µ—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å)
                    plt.axhline(y=distance_threshold, linestyle="--")
                if cut_distance is not None:
                    # –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ cut_distance
                    plt.axhline(y=cut_distance, linestyle=":", linewidth=1)
                plt.title("Hierarchical clustering dendrogram")
                plt.xlabel("Objects")
                plt.ylabel("Distance")
                if savefig_path:
                    plt.tight_layout()
                    plt.savefig(savefig_path, dpi=150)
                plt.show()

            # --- –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ cut_distance ---
            if cut_distance is not None:
                flat_clusters = sch.fcluster(
                    Z,
                    t=cut_distance,
                    criterion="distance"
                )
                cut_labels = pd.Series(
                    flat_clusters,
                    index=X.index,
                    name=f"cluster_d{cut_distance:g}",
                )

                if cut_label_column is None:
                    cut_label_column = f"hclust_{linkage}_cut_{cut_distance:g}"

                self.df[cut_label_column] = pd.Series(index=self.df.index, dtype="Int64")
                self.df.loc[cut_labels.index, cut_label_column] = cut_labels.values

        return labels, model, cut_labels


    def hierarchical_clustering_all_features(
        self,
        features: list[str],
        n_clusters: int = 4,
        *,
        linkage: str = "ward",         # "ward" | "complete" | "average" | "single"
        metric: str = "euclidean",     # –¥–ª—è "ward" –≤—Å–µ–≥–¥–∞ euclidean
        standardize: bool = True,
        label_column: str | None = None,
        plot: bool = True,
        savefig_path: str | None = None,
        top_categories: int = 3,       # —Å–∫–æ–ª—å–∫–æ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–∏—Ç—å
    ):
        """
        –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –í–°–ï–ú —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º (—á–∏—Å–ª–æ–≤—ã–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º).

        –®–∞–≥–∏:
        1. –î–µ–ª–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ.
        2. –ß–∏—Å–ª–æ–≤—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ -> OneHotEncoder.
        3. –î–µ–ª–∞–µ–º AgglomerativeClustering –ø–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º.
        4. –°—Ç—Ä–æ–∏–º –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º—É –ø–æ —Ç–µ–º –∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º.
        5. –î–ª—è n_clusters (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4) –≤—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º.

        –†–µ–∑—É–ª—å—Ç–∞—Ç:
        labels: pd.Series —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        model : AgglomerativeClustering
        summary: —Å–ª–æ–≤–∞—Ä—å —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """

        # --- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
        if not features:
            raise ValueError("–°–ø–∏—Å–æ–∫ features –ø—É—Å—Ç.")

        X = self.df[features].dropna()
        if X.empty:
            raise ValueError("–ü–æ—Å–ª–µ dropna() –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]
        cat_cols = [c for c in X.columns if c not in num_cols]

        if not num_cols and not cat_cols:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥: —á–∏—Å–ª–æ–≤—ã–µ + –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        numeric_transformer = StandardScaler() if standardize and num_cols else "passthrough"
        categorical_transformer = OneHotEncoder(
            handle_unknown="ignore",
            sparse_output=False,   # –≤–∞–∂–Ω–æ: –Ω—É–∂–µ–Ω –ø–ª–æ—Ç–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è scipy.linkage
        ) if cat_cols else "passthrough"

        preprocessor = ColumnTransformer(
            transformers=[
                ("num", numeric_transformer, num_cols),
                ("cat", categorical_transformer, cat_cols),
            ],
            remainder="drop",
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X_trans = preprocessor.fit_transform(X)

        # --- –º–æ–¥–µ–ª—å ---
        effective_metric = "euclidean" if linkage == "ward" else metric

        model = AgglomerativeClustering(
            n_clusters=n_clusters,
            linkage=linkage,
            metric=effective_metric,
        ).fit(X_trans)

        labels = pd.Series(model.labels_, index=X.index, name="cluster")

        # --- –∑–∞–ø–∏—Å—å –º–µ—Ç–æ–∫ –≤ df ---
        if label_column is None:
            label_column = f"hclust_all_{linkage}_{n_clusters}"
        self.df[label_column] = pd.Series(index=self.df.index, dtype="Int64")
        self.df.loc[labels.index, label_column] = labels.values

        # --- –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞) ---
        if plot:
            # linkage —Ç—Ä–µ–±—É–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —É–∂–µ —á–∏—Å–ª–æ–≤–æ–π –º–∞—Å—Å–∏–≤
            Z = sch.linkage(X_trans, method=linkage, metric=effective_metric)

            plt.figure(figsize=(12, 5))
            sch.dendrogram(Z, no_labels=True)
            plt.title(f"Hierarchical clustering dendrogram ({n_clusters} clusters)")
            plt.xlabel("Objects")
            plt.ylabel("Distance")

            # –ü—Ä–∏–º–µ—Ä–Ω–æ –ø–æ–∫–∞–∑–∏–º —É—Ä–æ–≤–µ–Ω—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π 4 –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            # (–¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ, –Ω–æ –Ω–∞–≥–ª—è–¥–Ω–æ)
            try:
                # –í—ã—Å–æ—Ç–∞ —Å–ª–∏—è–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è—é—â–∞—è n_clusters –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:
                # –±–µ—Ä—ë–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–∫–æ–ª–æ–Ω–∫–∞ 2) –Ω–∞ —à–∞–≥–µ, –∫–æ–≥–¥–∞ –æ—Å—Ç–∞—ë—Ç—Å—è n_clusters
                # –í Z shape = (n_samples-1, 4). –ë–µ—Ä—ë–º —ç–ª–µ–º–µ–Ω—Ç —Å –∏–Ω–¥–µ–∫—Å–æ–º -n_clusters.
                threshold = Z[-n_clusters, 2]
                plt.axhline(y=threshold, linestyle="--")
            except Exception:
                pass

            if savefig_path:
                plt.tight_layout()
                plt.savefig(savefig_path, dpi=150)
            plt.show()

        # --- –æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –ò–°–•–û–î–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–∞ –Ω–µ –≤ OneHot/StandardScaler) ---
        X_with_labels = X.copy()
        X_with_labels["cluster"] = labels

        summary: dict[int, dict] = {}

        for cl in sorted(labels.unique()):
            group = X_with_labels[X_with_labels["cluster"] == cl].drop(columns=["cluster"])
            size = len(group)

            print(f"\n=== –ö–ª–∞—Å—Ç–µ—Ä {cl} (n={size}) ===")

            cluster_info: dict[str, dict] = {
                "size": size,
                "numeric": {},
                "categorical": {},
            }

            # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: –±–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if num_cols:
                print("\n–ß–ò–°–õ–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò (describe):")
                desc = group[num_cols].describe().T
                print(desc)

                for col in num_cols:
                    cluster_info["numeric"][col] = {
                        "mean": group[col].mean(),
                        "std": group[col].std(),
                        "min": group[col].min(),
                        "max": group[col].max(),
                    }

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —Ç–æ–ø –∑–Ω–∞—á–µ–Ω–∏–π
            if cat_cols:
                print("\n–ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (top –∑–Ω–∞—á–µ–Ω–∏—è):")
                for col in cat_cols:
                    vc = group[col].value_counts(dropna=False)
                    top = vc.head(top_categories)
                    print(f"\n{col}:")
                    print(top)

                    cluster_info["categorical"][col] = top.to_dict()

            summary[int(cl)] = cluster_info

        return labels, model, summary


    
    
    def add_feature_ranking(self, target):
        import numpy as np
        import pandas as pd
        from pandas.api.types import (
            is_object_dtype, is_string_dtype, is_bool_dtype,
            is_numeric_dtype, is_integer_dtype, is_datetime64_any_dtype,
            CategoricalDtype,
        )
        from sklearn.compose import ColumnTransformer
        from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler
        from sklearn.impute import SimpleImputer
        from sklearn.pipeline import Pipeline
        from sklearn.model_selection import cross_val_score, KFold
        from sklearn.linear_model import LinearRegression, RidgeClassifier
        from sklearn.exceptions import ConvergenceWarning
        import warnings

        if not isinstance(target, str):
            raise ValueError("target must be a column name (str)")
        if target not in self.df.columns:
            raise ValueError(f"target column '{target}' not found in dataset")

        y = self.df[target]
        X_full = self.df.drop(columns=[target])

        def is_classification(series: pd.Series) -> bool:
            if not is_numeric_dtype(series):
                return True
            nun = series.nunique(dropna=True)
            if is_integer_dtype(series) and nun <= 12:
                return True
            if nun <= 6:
                return True
            return False

        task_is_clf = is_classification(y)
        scoring = "accuracy" if task_is_clf else "r2"
        estimator = RidgeClassifier(alpha=1.0) if task_is_clf else LinearRegression()
        cv = KFold(n_splits=5, shuffle=True, random_state=42)

        idx = y.dropna().index
        y = y.loc[idx]
        X_full = X_full.loc[idx]

        def build_preprocessor(cols: list[str]) -> ColumnTransformer:
            sub = X_full[cols]
            num_cols = [c for c in sub.columns if is_numeric_dtype(sub[c])]
            cat_cols = [c for c in sub.columns if is_object_dtype(sub[c]) or is_string_dtype(sub[c]) or isinstance(sub[c].dtype, CategoricalDtype) or is_bool_dtype(sub[c])]
            dt_cols = [c for c in sub.columns if is_datetime64_any_dtype(sub[c])]

            def dt_to_parts(X: pd.DataFrame) -> pd.DataFrame:
                X = X.copy()
                out = pd.DataFrame(index=X.index)
                for c in X.columns:
                    s = pd.to_datetime(X[c], errors="coerce")
                    out[f"{c}__year"] = s.dt.year.astype("float64")
                    out[f"{c}__month"] = s.dt.month.astype("float64")
                    out[f"{c}__day"] = s.dt.day.astype("float64")
                    out[f"{c}__dow"] = s.dt.dayofweek.astype("float64")
                return out

            num_tr = Pipeline([
                ("imp", SimpleImputer(strategy="median")),
                ("sc", StandardScaler(with_mean=False)),
            ])
            cat_tr = Pipeline([
                ("imp", SimpleImputer(strategy="most_frequent")),
                ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=True)),
            ])
            dt_tr = Pipeline([
                ("parts", FunctionTransformer(dt_to_parts, validate=False)),
                ("imp", SimpleImputer(strategy="median")),
                ("sc", StandardScaler(with_mean=False)),
            ])
            transformers = []
            if num_cols:
                transformers.append(("num", num_tr, num_cols))
            if cat_cols:
                transformers.append(("cat", cat_tr, cat_cols))
            if dt_cols:
                transformers.append(("dt", dt_tr, dt_cols))
            if not transformers:
                raise ValueError("No usable columns in the current subset")
            return ColumnTransformer(transformers)

        selected: list[str] = []
        remaining = list(X_full.columns)
        results = []
        prev_score = -np.inf

        while remaining:
            best_feat = None
            best_score = -np.inf
            for f in remaining:
                cols = selected + [f]
                pre = build_preprocessor(cols)
                pipe = Pipeline([("prep", pre), ("est", estimator)])
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore", category=ConvergenceWarning)
                        score = float(cross_val_score(pipe, X_full[cols], y, cv=cv, scoring=scoring).mean())
                except Exception:
                    score = -np.inf
                if score > best_score:
                    best_score = score
                    best_feat = f
            selected.append(best_feat)
            remaining.remove(best_feat)
            results.append({
                "feature": best_feat,
                "step": len(selected),
                "cv_score_after_add": best_score,
                "delta": (best_score - prev_score) if np.isfinite(prev_score) else np.nan,
            })
            prev_score = best_score

        return pd.DataFrame(results)


    def add_feature_ranking_global(self):

        features = list(self.df.columns)
        agg = {}
        for tgt in features:
            try:
                rank = self.add_feature_ranking(tgt)
            except Exception:
                continue
            if rank is None or len(rank) == 0:
                continue
            r = rank.copy()
            r["delta"] = r["delta"].fillna(0.0)
            for _, row in r.iterrows():
                f = row["feature"]
                d = float(row["delta"]) if pd.notnull(row["delta"]) else 0.0
                st = int(row["step"]) if pd.notnull(row["step"]) else 0
                if f not in agg:
                    agg[f] = {"sum_delta_pos": 0.0, "sum_delta": 0.0, "count": 0, "count_pos": 0, "steps": []}
                agg[f]["sum_delta"] += d
                agg[f]["count"] += 1
                if d > 0:
                    agg[f]["sum_delta_pos"] += d
                    agg[f]["count_pos"] += 1
                agg[f]["steps"].append(st)
        rows = []
        for f, s in agg.items():
            rows.append({
                "feature": f,
                "targets_covered": s["count"],
                "used_with_gain": s["count_pos"],
                "mean_delta": (s["sum_delta"] / s["count"]) if s["count"] else 0.0,
                "mean_delta_pos": (s["sum_delta_pos"] / s["count_pos"]) if s["count_pos"] else 0.0,
                "sum_delta_pos": s["sum_delta_pos"],
                "median_step": float(np.median(s["steps"])) if s["steps"] else np.nan,
            })
        out = pd.DataFrame(rows)
        if out.empty:
            return out
        out = out.sort_values(["sum_delta_pos", "mean_delta_pos", "used_with_gain"], ascending=[False, False, False]).reset_index(drop=True)
        return out
    
    
    def hierarchical_clustering_all_features_ordinal(
        self,
        features: list[str],
        n_clusters: int = 4,
        *,
        linkage: str = "ward",         # "ward" | "complete" | "average" | "single"
        metric: str = "euclidean",     # –¥–ª—è "ward" –≤—Å–µ–≥–¥–∞ euclidean
        standardize: bool = True,
        label_column: str | None = None,
        plot: bool = True,
        savefig_path: str | None = None,
        top_categories: int = 3,       # —Å–∫–æ–ª—å–∫–æ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–∏—Ç—å
    ):
        """
        –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –í–°–ï–ú –ø—Ä–∏–∑–Ω–∞–∫–∞–º:
        - —á–∏—Å–ª–æ–≤—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å (–º–æ–∂–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
        - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–¥–∏—Ä—É—é—Ç—Å—è —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ (ordinal encoding)

        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏–¥—ë—Ç –ø–æ —á–∏—Å–ª–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ, –∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
        """

        if not features:
            raise ValueError("–°–ø–∏—Å–æ–∫ features –ø—É—Å—Ç.")

        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        X = self.df[features].dropna()
        if X.empty:
            raise ValueError("–ü–æ—Å–ª–µ dropna() –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]
        cat_cols = [c for c in X.columns if c not in num_cols]

        if not num_cols and not cat_cols:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

        # --- –ö–û–î–ò–†–û–í–ê–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –í –ß–ò–°–õ–ê ---
        X_enc = X.copy()
        encoding_maps: dict[str, dict[int, object]] = {}  # col -> {code: original_value}

        for col in cat_cols:
            # factorize –¥–∞—ë—Ç –∫–æ–¥ (0..k-1) –∏ –º–∞—Å—Å–∏–≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            codes, uniques = pd.factorize(X[col], sort=True)
            X_enc[col] = codes.astype(float)   # float, —á—Ç–æ–±—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–ª StandardScaler
            encoding_maps[col] = dict(enumerate(uniques))

        # --- –ú–ê–°–°–ò–í –î–õ–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò ---
        use_cols = num_cols + cat_cols
        X_values = X_enc[use_cols].values

        if standardize:
            scaler = StandardScaler()
            X_values = scaler.fit_transform(X_values)

        # --- –ú–û–î–ï–õ–¨ ---
        effective_metric = "euclidean" if linkage == "ward" else metric

        model = AgglomerativeClustering(
            n_clusters=n_clusters,
            linkage=linkage,
            metric=effective_metric,
        ).fit(X_values)

        labels = pd.Series(model.labels_, index=X.index, name="cluster")

        # --- –ó–ê–ü–ò–°–¨ –ú–ï–¢–û–ö –í df ---
        if label_column is None:
            label_column = f"hclust_ord_{linkage}_{n_clusters}"

        self.df[label_column] = pd.Series(index=self.df.index, dtype="Int64")
        self.df.loc[labels.index, label_column] = labels.values

        # --- –î–ï–ù–î–†–û–ì–†–ê–ú–ú–ê (–û–°–¢–û–†–û–ñ–ù–û: O(n^2) –ü–û –°–¢–†–û–ö–ê–ú) ---
        if plot:
            Z = sch.linkage(X_values, method=linkage, metric=effective_metric)

            plt.figure(figsize=(12, 5))
            sch.dendrogram(Z, no_labels=True)
            plt.title(f"Hierarchical clustering dendrogram ({n_clusters} clusters)")
            plt.xlabel("Objects")
            plt.ylabel("Distance")

            # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è n_clusters
            try:
                threshold = Z[-n_clusters, 2]
                plt.axhline(y=threshold, linestyle="--")
            except Exception:
                pass

            if savefig_path:
                plt.tight_layout()
                plt.savefig(savefig_path, dpi=150)
            plt.show()

        # --- –û–ü–ò–°–ê–ù–ò–ï –ö–õ–ê–°–¢–ï–†–û–í –í –ò–°–•–û–î–ù–´–• –ü–†–ò–ó–ù–ê–ö–ê–• ---
        X_with_labels = X.copy()  # –ò–°–•–û–î–ù–´–ï –∑–Ω–∞—á–µ–Ω–∏—è (—Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞)
        X_with_labels["cluster"] = labels

        summary: dict[int, dict] = {}

        for cl in sorted(labels.unique()):
            group = X_with_labels[X_with_labels["cluster"] == cl].drop(columns=["cluster"])
            size = len(group)

            print(f"\n=== –ö–ª–∞—Å—Ç–µ—Ä {cl} (n={size}) ===")

            cluster_info: dict[str, dict] = {
                "size": size,
                "numeric": {},
                "categorical": {},
            }
            
            # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            if num_cols:
                print("\n–ß–ò–°–õ–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò (describe):")
                desc = group[num_cols].describe().T
                print(desc)

                for col in num_cols:
                    cluster_info["numeric"][col] = {
                        "mean": group[col].mean(),
                        "std": group[col].std(),
                        "min": group[col].min(),
                        "max": group[col].max(),
                    }

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            if cat_cols:
                print("\n–ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (top –∑–Ω–∞—á–µ–Ω–∏—è):")
                for col in cat_cols:
                    vc = group[col].value_counts(dropna=False)
                    top = vc.head(top_categories)
                    print(f"\n{col}:")
                    print(top)

                    cluster_info["categorical"][col] = top.to_dict()

            summary[int(cl)] = cluster_info

        # encoding_maps —Ç—É—Ç –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π ‚Äî –≤–¥—Ä—É–≥ –∑–∞—Ö–æ—á–µ—à—å —è–≤–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–¥—ã
        return labels, model, summary, encoding_maps


# ============================================
# ‚öôÔ∏è Global Functions
# ============================================


def create_data_with_gaps(data: dict[str, Dataset], percents: list[float]):
    for p in percents:
        deleted_small = data["small"].remove_blocks(percent=p, inplace=False)
        deleted_medium = data["medium"].remove_blocks(percent=p, inplace=False)
        deleted_large = data["large"].remove_blocks(percent=p, inplace=False)
        
        deleted_small.to_excel(f"{DELETED_SMALL_PATH}/{int(p*100)}.xlsx", index=False)
        deleted_medium.to_excel(f"{DELETED_MEDIUM_PATH}/{int(p*100)}.xlsx", index=False)
        deleted_large.to_excel(f"{DELETED_LARGE_PATH}/{int(p*100)}.xlsx", index=False)
        
        
def recover_data():
    data = {
        "small": [(Dataset(f"out/data_with_gaps/small/{int(p*100)}.xlsx"), int(p*100)) for p in PERCENTS_OF_GAPS],
        "medium": [(Dataset(f"out/data_with_gaps/medium/{int(p*100)}.xlsx"), int(p*100)) for p in PERCENTS_OF_GAPS],
        "large": [(Dataset(f"out/data_with_gaps/large/{int(p*100)}.xlsx"), int(p*100)) for p in PERCENTS_OF_GAPS],
    }
    
    #-------------Group Algorithm----------------
    for size, datasets in data.items():
        for (dataset, p) in datasets:
            dataset_inputed = dataset.inpute_groups()
            dataset_inputed.to_excel(f"out/recovered_groups/{size}/{p}.xlsx", index=False)
            
    #-------------Zet Algorithm----------------        
    for size, datasets in data.items():
        for (dataset, p) in datasets:
            dataset_inputed = dataset.impute_zet(k=15)
            dataset_inputed.to_excel(f"out/recovered_zet/{size}/{p}.xlsx", index=False)
            
            
def clustering_ward():
    data = Dataset("out/recovered_groups/small/3.xlsx")
    
    labels, model = data.hierarchical_clustering(
        features=["store_name", "date-time", "coordinates", "brands", "price", "cards_number", "number_of_products", "receipt_id", "total_cost"],
        n_clusters=3,          # –∏–ª–∏ distance_threshold=..., —Ç–æ–≥–¥–∞ n_clusters –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
        linkage="ward",        # "ward" | "complete" | "average" | "single"
        metric="euclidean",    # –¥–ª—è non-ward –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å, –Ω–∞–ø—Ä. "cosine"
        standardize=True,
        label_column="cluster3",
        plot=True,
        savefig_path=None,
    )
    
    
def clustering_chebyshev():
    data = Dataset("out/recovered_groups/small/3.xlsx")
    
    labels, model = data.hierarchical_clustering(
        features=["store_name", "date-time", "coordinates", "brands", "price", "cards_number", "number_of_products", "receipt_id", "total_cost"],
        n_clusters=3,          # –∏–ª–∏ distance_threshold=..., —Ç–æ–≥–¥–∞ n_clusters –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
        linkage="complete",        # "ward" | "complete" | "average" | "single"
        metric="chebyshev",    # –¥–ª—è non-ward –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å, –Ω–∞–ø—Ä. "cosine"
        standardize=True,
        label_column="cluster3",
        plot=True,
        savefig_path=None,
    )
    
    
def cluster_ward():
    data = Dataset(SMALL_PATH)
    
    labels, model, summary, enc_maps = data.hierarchical_clustering_all_features_ordinal(
        features=["store_name", "date-time", "coordinates","categories", "brands", "price", "cards_number", "number_of_products", "receipt_id", "total_cost"],              # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        n_clusters=4,
        linkage="ward",
        standardize=True,
        plot=False,                  # –í–ê–ñ–ù–û: –±–µ–∑ –¥–µ–Ω–¥–æ–≥—Ä–∞–º–º—ã
    )


        
        
def cluster_chebyshev():
    data = Dataset(SMALL_PATH)
    
    labels, model, summary, enc_maps = data.hierarchical_clustering_all_features_ordinal(
        features=["store_name", "date-time", "coordinates","categories", "brands", "price", "cards_number", "number_of_products", "receipt_id", "total_cost"],              # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        n_clusters=4,
        linkage="complete",        # "ward" | "complete" | "average" | "single"
        metric="chebyshev",
        standardize=True,
        plot=False,                  # –í–ê–ñ–ù–û: –±–µ–∑ –¥–µ–Ω–¥–æ–≥—Ä–∞–º–º—ã
    )


    
    
def Add_method():
    data = Dataset(SMALL_PATH)
    
    rank = data.add_feature_ranking(target="receipt_id")
    
    print(rank)
    
    
def inf_cluster_ward():
    data = Dataset(SMALL_PATH)
    
    labels, model, summary, enc_maps = data.hierarchical_clustering_all_features_ordinal(
        features=["store_name", "brands", "price", "cards_number", "total_cost"],              # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        n_clusters=4,
        linkage="ward",
        standardize=True,
        plot=True,                  # –í–ê–ñ–ù–û: –±–µ–∑ –¥–µ–Ω–¥–æ–≥—Ä–∞–º–º—ã
    )
    
    
def inf_cluster_chebyshev(path):
    data = Dataset(path)
    
    labels, model, summary, enc_maps = data.hierarchical_clustering_all_features_ordinal(
        features=["store_name", "brands", "price", "cards_number", "total_cost"],              # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        n_clusters=4,
        linkage="complete",        # "ward" | "complete" | "average" | "single"
        metric="chebyshev",
        standardize=True,
        plot=True,                  # –í–ê–ñ–ù–û: –±–µ–∑ –¥–µ–Ω–¥–æ–≥—Ä–∞–º–º—ã
    )
    
    
def inf_cluster_ward_recovered(path):
    data = Dataset(path)
    
    labels, model, summary, enc_maps = data.hierarchical_clustering_all_features_ordinal(
        features=["store_name", "brands", "price", "cards_number", "total_cost"],              # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        n_clusters=4,
        linkage="ward",
        standardize=True,
        plot=True,                  # –í–ê–ñ–ù–û: –±–µ–∑ –¥–µ–Ω–¥–æ–≥—Ä–∞–º–º—ã
    )
        
        
# ============================================
# üì• Loaded Data
# ============================================


data = {
    "small": Dataset(SMALL_PATH),
#     "medium": Dataset(MEDIUM_PATH),
#     "large": Dataset(LARGE_PATH),
}

if __name__ == "__main__":
    # print("Feature, informativity")
    # print("store_name         0.76")
    # print("coordinates        0.71")
    # print("categories         0.65")
    # print("")
    
    Add_method()
